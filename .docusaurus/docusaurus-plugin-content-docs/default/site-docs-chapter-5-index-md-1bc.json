{
  "id": "chapter-5/index",
  "title": "Chapter 5: Vision-Language-Action Pipelines",
  "description": "This chapter explores Vision-Language-Action (VLA) pipelines that integrate perception, language understanding, and robotic action for intelligent humanoid systems using frameworks like Whisper, LLMs, and ROS actions.",
  "source": "@site/docs/chapter-5/index.md",
  "sourceDirName": "chapter-5",
  "slug": "/chapter-5/",
  "permalink": "/physical-ai-book/docs/chapter-5/",
  "draft": false,
  "unlisted": false,
  "editUrl": "https://github.com/your-username/physical-ai-book/tree/main/docs/chapter-5/index.md",
  "tags": [],
  "version": "current",
  "sidebarPosition": 1,
  "frontMatter": {
    "sidebar_position": 1
  },
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Chapter 5: Vision-Language-Action Pipelines",
    "permalink": "/physical-ai-book/docs/category/chapter-5-vision-language-action-pipelines"
  },
  "next": {
    "title": "LLM-ROS Integration for Action Execution",
    "permalink": "/physical-ai-book/docs/chapter-5/llm-ros-actions"
  }
}